{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/huggingface/lerobot/blob/main/examples/1_load_lerobot_dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Returning existing local_dir `..\\data\\dataset\\exam_2` as remote repo cannot be accessed in `snapshot_download` (None).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of episodes: 10\n",
      "Average number of frames per episode: 1436.300\n",
      "Frames per second used during data collection: 30\n",
      "Robot type: koch\n",
      "keys to access images from cameras: ds_meta.camera_keys=['observation.images.top', 'observation.images.side', 'observation.images.arm']\n",
      "\n",
      "Tasks:\n",
      "{0: '将篮子放到容器里'}\n",
      "Features:\n",
      "{'action': {'dtype': 'float32',\n",
      "            'names': ['main_shoulder_pan',\n",
      "                      'main_shoulder_lift',\n",
      "                      'main_elbow_flex',\n",
      "                      'main_wrist_flex',\n",
      "                      'main_wrist_roll',\n",
      "                      'main_gripper'],\n",
      "            'shape': (6,)},\n",
      " 'episode_index': {'dtype': 'int64', 'names': None, 'shape': (1,)},\n",
      " 'frame_index': {'dtype': 'int64', 'names': None, 'shape': (1,)},\n",
      " 'index': {'dtype': 'int64', 'names': None, 'shape': (1,)},\n",
      " 'observation.images.arm': {'dtype': 'video',\n",
      "                            'info': {'has_audio': False,\n",
      "                                     'video.channels': 3,\n",
      "                                     'video.codec': 'av1',\n",
      "                                     'video.fps': 30.0,\n",
      "                                     'video.height': 480,\n",
      "                                     'video.is_depth_map': False,\n",
      "                                     'video.pix_fmt': 'yuv420p',\n",
      "                                     'video.width': 640},\n",
      "                            'names': ['height', 'width', 'channels'],\n",
      "                            'shape': (480, 640, 3)},\n",
      " 'observation.images.side': {'dtype': 'video',\n",
      "                             'info': {'has_audio': False,\n",
      "                                      'video.channels': 3,\n",
      "                                      'video.codec': 'av1',\n",
      "                                      'video.fps': 30.0,\n",
      "                                      'video.height': 480,\n",
      "                                      'video.is_depth_map': False,\n",
      "                                      'video.pix_fmt': 'yuv420p',\n",
      "                                      'video.width': 640},\n",
      "                             'names': ['height', 'width', 'channels'],\n",
      "                             'shape': (480, 640, 3)},\n",
      " 'observation.images.top': {'dtype': 'video',\n",
      "                            'info': {'has_audio': False,\n",
      "                                     'video.channels': 3,\n",
      "                                     'video.codec': 'av1',\n",
      "                                     'video.fps': 30.0,\n",
      "                                     'video.height': 480,\n",
      "                                     'video.is_depth_map': False,\n",
      "                                     'video.pix_fmt': 'yuv420p',\n",
      "                                     'video.width': 640},\n",
      "                            'names': ['height', 'width', 'channels'],\n",
      "                            'shape': (480, 640, 3)},\n",
      " 'observation.state': {'dtype': 'float32',\n",
      "                       'names': ['main_shoulder_pan',\n",
      "                                 'main_shoulder_lift',\n",
      "                                 'main_elbow_flex',\n",
      "                                 'main_wrist_flex',\n",
      "                                 'main_wrist_roll',\n",
      "                                 'main_gripper'],\n",
      "                       'shape': (6,)},\n",
      " 'task_index': {'dtype': 'int64', 'names': None, 'shape': (1,)},\n",
      " 'timestamp': {'dtype': 'float32', 'names': None, 'shape': (1,)}}\n",
      "LeRobotDatasetMetadata({\n",
      "    Repository ID: 'Haskely/exam_2',\n",
      "    Total episodes: '10',\n",
      "    Total frames: '14363',\n",
      "    Features: '['action', 'observation.state', 'observation.images.top', 'observation.images.side', 'observation.images.arm', 'timestamp', 'frame_index', 'episode_index', 'index', 'task_index']',\n",
      "})',\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script demonstrates the use of `LeRobotDataset` class for handling and processing robotic datasets from Hugging Face.\n",
    "It illustrates how to load datasets, manipulate them, and apply transformations suitable for machine learning tasks in PyTorch.\n",
    "\n",
    "Features included in this script:\n",
    "- Viewing a dataset's metadata and exploring its properties.\n",
    "- Loading an existing dataset from the hub or a subset of it.\n",
    "- Accessing frames by episode number.\n",
    "- Using advanced dataset features like timestamp-based frame selection.\n",
    "- Demonstrating compatibility with PyTorch DataLoader for batch processing.\n",
    "\n",
    "The script ends with examples of how to batch process data using PyTorch's DataLoader.\n",
    "\"\"\"\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDatasetMetadata\n",
    "\n",
    "# Or simply explore them in your web browser directly at:\n",
    "# https://huggingface.co/datasets?other=LeRobot\n",
    "\n",
    "# Let's take this one for this example\n",
    "repo_id = \"Haskely/exam_2\"\n",
    "kwargs = dict(\n",
    "    root=\"../data/dataset/exam_2\",\n",
    "    local_files_only=True,\n",
    ")\n",
    "# We can have a look and fetch its metadata to know more about it:\n",
    "ds_meta = LeRobotDatasetMetadata(repo_id, **kwargs)\n",
    "\n",
    "# By instantiating just this class, you can quickly access useful information about the content and the\n",
    "# structure of the dataset without downloading the actual data yet (only metadata files — which are\n",
    "# lightweight).\n",
    "print(f\"Total number of episodes: {ds_meta.total_episodes}\")\n",
    "print(\n",
    "    f\"Average number of frames per episode: {ds_meta.total_frames / ds_meta.total_episodes:.3f}\"\n",
    ")\n",
    "print(f\"Frames per second used during data collection: {ds_meta.fps}\")\n",
    "print(f\"Robot type: {ds_meta.robot_type}\")\n",
    "print(f\"keys to access images from cameras: {ds_meta.camera_keys=}\\n\")\n",
    "\n",
    "print(\"Tasks:\")\n",
    "print(ds_meta.tasks)\n",
    "print(\"Features:\")\n",
    "pprint(ds_meta.features)\n",
    "\n",
    "# You can also get a short summary by simply printing the object:\n",
    "print(ds_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Returning existing local_dir `..\\data\\dataset\\exam_2` as remote repo cannot be accessed in `snapshot_download` (None).\n",
      "Returning existing local_dir `..\\data\\dataset\\exam_2` as remote repo cannot be accessed in `snapshot_download` (None).\n",
      "Returning existing local_dir `..\\data\\dataset\\exam_2` as remote repo cannot be accessed in `snapshot_download` (None).\n",
      "Returning existing local_dir `..\\data\\dataset\\exam_2` as remote repo cannot be accessed in `snapshot_download` (None).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected episodes: [0, 7]\n",
      "Number of episodes selected: 2\n",
      "Number of frames selected: 3560\n",
      "Number of episodes selected: 10\n",
      "Number of frames selected: 14363\n",
      "LeRobotDatasetMetadata({\n",
      "    Repository ID: 'Haskely/exam_2',\n",
      "    Total episodes: '10',\n",
      "    Total frames: '14363',\n",
      "    Features: '['action', 'observation.state', 'observation.images.top', 'observation.images.side', 'observation.images.arm', 'timestamp', 'frame_index', 'episode_index', 'index', 'task_index']',\n",
      "})',\n",
      "\n",
      "Dataset({\n",
      "    features: ['action', 'observation.state', 'timestamp', 'frame_index', 'episode_index', 'index', 'task_index'],\n",
      "    num_rows: 14363\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# You can then load the actual dataset from the hub.\n",
    "# Either load any subset of episodes:\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "\n",
    "dataset = LeRobotDataset(repo_id, episodes=[0, 7], **kwargs)\n",
    "\n",
    "# And see how many frames you have:\n",
    "print(f\"Selected episodes: {dataset.episodes}\")\n",
    "print(f\"Number of episodes selected: {dataset.num_episodes}\")\n",
    "print(f\"Number of frames selected: {dataset.num_frames}\")\n",
    "\n",
    "# Or simply load the entire dataset:\n",
    "dataset = LeRobotDataset(repo_id, **kwargs)\n",
    "print(f\"Number of episodes selected: {dataset.num_episodes}\")\n",
    "print(f\"Number of frames selected: {dataset.num_frames}\")\n",
    "\n",
    "# The previous metadata class is contained in the 'meta' attribute of the dataset:\n",
    "print(dataset.meta)\n",
    "\n",
    "# LeRobotDataset actually wraps an underlying Hugging Face dataset\n",
    "# (see https://huggingface.co/docs/datasets for more information).\n",
    "print(dataset.hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from': tensor([    0,  1953,  3364,  4692,  6032,  7266,  9015, 10550, 12157, 13463]),\n",
       " 'to': tensor([ 1953,  3364,  4692,  6032,  7266,  9015, 10550, 12157, 13463, 14363])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.episode_data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_index = 0\n",
    "from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "to_idx = dataset.episode_data_index[\"to\"][episode_index].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': {'dtype': 'float32',\n",
       "  'shape': (6,),\n",
       "  'names': ['main_shoulder_pan',\n",
       "   'main_shoulder_lift',\n",
       "   'main_elbow_flex',\n",
       "   'main_wrist_flex',\n",
       "   'main_wrist_roll',\n",
       "   'main_gripper']},\n",
       " 'observation.state': {'dtype': 'float32',\n",
       "  'shape': (6,),\n",
       "  'names': ['main_shoulder_pan',\n",
       "   'main_shoulder_lift',\n",
       "   'main_elbow_flex',\n",
       "   'main_wrist_flex',\n",
       "   'main_wrist_roll',\n",
       "   'main_gripper']},\n",
       " 'observation.images.top': {'dtype': 'video',\n",
       "  'shape': (480, 640, 3),\n",
       "  'names': ['height', 'width', 'channels'],\n",
       "  'info': {'video.fps': 30.0,\n",
       "   'video.height': 480,\n",
       "   'video.width': 640,\n",
       "   'video.channels': 3,\n",
       "   'video.codec': 'av1',\n",
       "   'video.pix_fmt': 'yuv420p',\n",
       "   'video.is_depth_map': False,\n",
       "   'has_audio': False}},\n",
       " 'observation.images.side': {'dtype': 'video',\n",
       "  'shape': (480, 640, 3),\n",
       "  'names': ['height', 'width', 'channels'],\n",
       "  'info': {'video.fps': 30.0,\n",
       "   'video.height': 480,\n",
       "   'video.width': 640,\n",
       "   'video.channels': 3,\n",
       "   'video.codec': 'av1',\n",
       "   'video.pix_fmt': 'yuv420p',\n",
       "   'video.is_depth_map': False,\n",
       "   'has_audio': False}},\n",
       " 'observation.images.arm': {'dtype': 'video',\n",
       "  'shape': (480, 640, 3),\n",
       "  'names': ['height', 'width', 'channels'],\n",
       "  'info': {'video.fps': 30.0,\n",
       "   'video.height': 480,\n",
       "   'video.width': 640,\n",
       "   'video.channels': 3,\n",
       "   'video.codec': 'av1',\n",
       "   'video.pix_fmt': 'yuv420p',\n",
       "   'video.is_depth_map': False,\n",
       "   'has_audio': False}},\n",
       " 'timestamp': {'dtype': 'float32', 'shape': (1,), 'names': None},\n",
       " 'frame_index': {'dtype': 'int64', 'shape': (1,), 'names': None},\n",
       " 'episode_index': {'dtype': 'int64', 'shape': (1,), 'names': None},\n",
       " 'index': {'dtype': 'int64', 'shape': (1,), 'names': None},\n",
       " 'task_index': {'dtype': 'int64', 'shape': (1,), 'names': None}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.meta.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeRobot datasets also subclasses PyTorch datasets so you can do everything you know and love from working\n",
    "# with the latter, like iterating through the dataset.\n",
    "# The __getitem__ iterates over the frames of the dataset. Since our datasets are also structured by\n",
    "# episodes, you can access the frame indices of any episode using the episode_data_index. Here, we access\n",
    "# frame indices associated to the first episode:\n",
    "episode_index = 0\n",
    "from_idx = dataset.episode_data_index[\"from\"][episode_index].item()\n",
    "to_idx = dataset.episode_data_index[\"to\"][episode_index].item()\n",
    "\n",
    "# Then we grab all the image frames from the first camera:\n",
    "camera_key = dataset.meta.camera_keys[0]\n",
    "frames = [dataset[idx][camera_key] for idx in range(from_idx, to_idx)]\n",
    "\n",
    "# The objects returned by the dataset are all torch.Tensors\n",
    "print(type(frames[0]))\n",
    "print(frames[0].shape)\n",
    "\n",
    "# Since we're using pytorch, the shape is in pytorch, channel-first convention (c, h, w).\n",
    "# We can compare this shape with the information available for that feature\n",
    "pprint(dataset.features[camera_key])\n",
    "# In particular:\n",
    "print(dataset.features[camera_key][\"shape\"])\n",
    "# The shape is in (h, w, c) which is a more universal format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
